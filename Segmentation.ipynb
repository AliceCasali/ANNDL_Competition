{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Segmentation with U-net\n",
    "## Best score 0.52563\n",
    "Which has been obtained in this way:\n",
    "- training the NN for 60 epochs with batchnormalization and dropout.\n",
    "- we reach a plateau. The validation scorse was stable.\n",
    "- so the last 20 epochs we use the dropout only at the last layer of the network, this increased the score reaching 0.52563\n",
    "\n",
    "Other details in the specific cells."
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example: Image Segmentation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We decided to use only horizontal and vertical flip for data augumentation because changing too much the images led to worst predictions."
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# ImageDataGenerator\n",
    "# ------------------\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_img_data_gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        fill_mode='reflect')\n",
    "\n",
    "train_mask_data_gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                         vertical_flip=True,\n",
    "                                         fill_mode='reflect',\n",
    "                                         rescale=1./255)\n",
    "\n",
    "valid_img_data_gen = ImageDataGenerator()\n",
    "valid_mask_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import shutil, random, json\n",
    "\n",
    "RESET_VALIDATION = True                    # if True the split of train and validation is run again randomly, if False the split remain the same\n",
    "\n",
    "DATASET_SPLIT = 0.8\n",
    "\n",
    "path = '/kaggle/working/tmp'\n",
    "if not os.path.exists(path) or RESET_VALIDATION:                # if the split doesn't exists OR if we want to create a new split, the new split is generated, otherwise not\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    if not os.path.exists(path+'/training'):\n",
    "        os.mkdir(path+'/training')\n",
    "    if not os.path.exists(path+'/validation'):\n",
    "        os.mkdir(path+'/validation')\n",
    "\n",
    "    source = \"/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset/training\"\n",
    "\n",
    "    # Destination paths\n",
    "    dest_train = path+'/training'\n",
    "    dest_valid = path+'/validation'\n",
    "\n",
    "    # dictionary to show the split with the format shown in the Evaluation tab\n",
    "    dataset_split = {}  \n",
    "    dataset_split[\"training\"] = []\n",
    "    dataset_split[\"validation\"] = []\n",
    "\n",
    "    if not os.path.exists(dest_train+'/'+'images'):\n",
    "        os.mkdir(dest_train+'/'+'images')\n",
    "    if not os.path.exists(dest_train+'/'+'images/img'):\n",
    "        os.mkdir(dest_train+'/'+'images/img')\n",
    "    if not os.path.exists(dest_valid+'/'+'images'):\n",
    "        os.mkdir(dest_valid+'/'+'images')\n",
    "    if not os.path.exists(dest_valid+'/'+'images/img'):\n",
    "        os.mkdir(dest_valid+'/'+'images/img')\n",
    "\n",
    "\n",
    "    if not os.path.exists(dest_train+'/'+'masks'):\n",
    "        os.mkdir(dest_train+'/'+'masks')\n",
    "    if not os.path.exists(dest_train+'/'+'masks/img'):\n",
    "        os.mkdir(dest_train+'/'+'masks/img')\n",
    "    if not os.path.exists(dest_valid+'/'+'masks'):\n",
    "        os.mkdir(dest_valid+'/'+'masks')\n",
    "    if not os.path.exists(dest_valid+'/'+'masks/img'):\n",
    "        os.mkdir(dest_valid+'/'+'masks/img')\n",
    "\n",
    "    #for images\n",
    "    source_images = source+'/images/img'\n",
    "    source_masks = source+'/masks/img'\n",
    "    files = os.listdir(source_images)\n",
    "    random.shuffle(files)\n",
    "    #create training set randomly\n",
    "    for i in range(int(len(files)*DATASET_SPLIT)):\n",
    "        dest = shutil.copy(source_images+'/'+files[i], dest_train+'/images/img/'+files[i])\n",
    "        dest = shutil.copy(source_masks+'/'+files[i], dest_train+'/masks/img/'+files[i])\n",
    "        dataset_split[\"training\"].append(files[i])\n",
    "    #create validation set randomly\n",
    "    for j in range(i+1, len(files)):\n",
    "        dest = shutil.copy(source_images+'/'+files[j], dest_valid+'/images/img/'+files[j])\n",
    "        dest = shutil.copy(source_masks+'/'+files[j], dest_valid+'/masks/img/'+files[j])\n",
    "        dataset_split[\"validation\"].append(files[j])\n",
    "\n",
    "    # create the json file using the dictionary dataset_split\n",
    "    with open('dataset_split.json', 'w') as fp:\n",
    "        json.dump(dataset_split, fp)\n",
    "\n",
    "print(\"Images uploaded for training:\", len(os.listdir(dest_train+'/images/img')))\n",
    "print(\"Images uploaded for validation:\", len(os.listdir(dest_valid+'/images/img')))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Create generators to read images from dataset directory\n",
    "# -------------------------------------------------------\n",
    "dataset_dir = '/kaggle/working/tmp'\n",
    "########################################changed batch size 12 from 4\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# img shape\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "classes = ['background',    # 0\n",
    "           'building',      # 1\n",
    "          ]\n",
    "\n",
    "\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "\n",
    "train_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None, # Because we have no class subfolders in this case\n",
    "                                                       shuffle=True,\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       seed=SEED)  \n",
    "\n",
    "train_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs,\n",
    "                                                         class_mode=None, # Because we have no class subfolders in this case\n",
    "                                                         shuffle=True,\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         color_mode = 'grayscale',\n",
    "                                                         seed=SEED)\n",
    "train_gen = zip(train_img_gen, train_mask_gen)\n",
    "\n",
    "validation_dir = os.path.join(dataset_dir, 'validation')\n",
    "valid_img_gen = valid_img_data_gen.flow_from_directory(os.path.join(validation_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None, # Because we have no class subfolders in this case\n",
    "                                                       shuffle=False,\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       seed=SEED)\n",
    "valid_mask_gen = valid_mask_data_gen.flow_from_directory(os.path.join(validation_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs, \n",
    "                                                         class_mode=None, # Because we have no class subfolders in this case\n",
    "                                                         shuffle=False,\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         color_mode = 'grayscale',\n",
    "                                                         seed=SEED)\n",
    "valid_gen = zip(valid_img_gen, valid_mask_gen)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Create Dataset objects\n",
    "# ----------------------\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "\n",
    "def prepare_target(x_, y_):\n",
    "    y_ = tf.cast(y_, tf.float32)\n",
    "    return x_, y_\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_target)\n",
    "\n",
    "# Repeat\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "# Validation\n",
    "# ----------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "valid_dataset = valid_dataset.map(prepare_target)\n",
    "\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.repeat()\n",
    "\n",
    "# Test\n",
    "# ----\n",
    "test_dataset = tf.data.Dataset.from_generator(lambda: test_gen,\n",
    "                                              output_types=(tf.float32, tf.float32),\n",
    "                                              output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "test_dataset = test_dataset.map(prepare_target)\n",
    "\n",
    "# Repeat\n",
    "test_dataset = valid_dataset.repeat()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "### Encoder-Decoder\n",
    "Here we decide to implement a U-Net which is recommended for segmentation problems. At first using only dropout, than we realized that a batch normalization led to better score."
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# Build U-Net model\n",
    "inputs = tf.keras.layers.Input((img_h, img_w, 3))\n",
    "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(s)\n",
    "c1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "c1 = tf.keras.layers.Activation(\"relu\")(c1)\n",
    "#c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c1)\n",
    "\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(p1)\n",
    "c2 = tf.keras.layers.BatchNormalization()(c2)\n",
    "c2 = tf.keras.layers.Activation(\"relu\")(c2)\n",
    "#c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c2)\n",
    "\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(p2)\n",
    "c3 = tf.keras.layers.BatchNormalization()(c3)\n",
    "c3 = tf.keras.layers.Activation(\"relu\")(c3)\n",
    "#c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c3)\n",
    "\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(p3)\n",
    "c4 = tf.keras.layers.BatchNormalization()(c4)\n",
    "c4 = tf.keras.layers.Activation(\"relu\")(c4)\n",
    "#c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c4)\n",
    "\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(p4)    \n",
    "c5 = tf.keras.layers.BatchNormalization()(c5)\n",
    "c5 = tf.keras.layers.Activation(\"relu\")(c5)\n",
    "#c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c5)\n",
    "\n",
    "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(u6)\n",
    "c6 = tf.keras.layers.BatchNormalization()(c6)\n",
    "c6 = tf.keras.layers.Activation(\"relu\")(c6)\n",
    "#c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(128, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c6)\n",
    "\n",
    "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(u7)\n",
    "c7 = tf.keras.layers.BatchNormalization()(c7)\n",
    "c7 = tf.keras.layers.Activation(\"relu\")(c7)\n",
    "#c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(u8)\n",
    "#c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(u9)  \n",
    "c9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "c9 = tf.keras.layers.Activation(\"relu\")(c9)\n",
    "c9 = tf.keras.layers.Dropout(0.2)(c9)\n",
    "\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_normal',\n",
    "                            padding='same')(c9)       \n",
    "c9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "c9 = tf.keras.layers.Activation(\"relu\")(c9)\n",
    "\n",
    "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare the model for training\n",
    "We saw increasing score using:\n",
    "- dice_loss for the loss\n",
    "- Adam optimizer with learning rate 0.0001\n",
    "- IoU metric"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "def my_IoU(y_true, y_pred):\n",
    "    # from pobability to predicted class {0, 1}\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)              # when using sigmoid. Use argmax for softmax\n",
    "\n",
    "    # A and B\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    # A or B\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    # IoU\n",
    "    return intersection / union\n",
    "\n",
    "metrics = [my_IoU]\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_loss, metrics=metrics)\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training with callbacks\n",
    "To have better scores we saved the weights, and re-train the net using the old weights. Our best score is 0.52563 which has been obtained in this way:\n",
    "- training the net 60 times with batchnormalization and dropout.\n",
    "- we reach a plateau. The validation scorse was stable.\n",
    "- so the last 20 epochs we use the dropout only at the last layer of the network, this increased the score reaching 0.52563"
   ]
  },
  {
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'segmentation_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "\n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "if 'my_model_weights.h5' in os.listdir('/kaggle/working'):\n",
    "    model.load_weights('my_model_weights.h5')\n",
    "\n",
    "history = model.fit(x=train_dataset,\n",
    "          epochs=60,  #### set repeat in training dataset\n",
    "          steps_per_epoch=len(train_img_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_img_gen), \n",
    "          callbacks=callbacks)\n",
    "\n",
    "saved_weights = model.save_weights('my_model_weights.h5')\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute prediction"
  },
  {
   "metadata": {
    "scrolled": false,
    "trusted": true
   },
   "cell_type": "code",
   "source": "import time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom PIL import Image\n\n%matplotlib notebook\n\n# define funct that riceive the path of an image and compute the groundtruth   \ndef predict_ground_truth(image_path):\n    img = Image.open(image_path)\n    img = img.resize((img_h, img_w))\n\n    img_arr = np.expand_dims(np.array(img), 0)\n\n    out_sigmoid = model.predict(x=img_arr / 1.)\n\n    # Get predicted class as the index corresponding to the maximum value in the vector probability\n    predicted_class = np.ones(shape=(img_w, img_h), dtype=np.int64)\n\n    for j in range(len(out_sigmoid[0])):\n        for k in range(len(out_sigmoid[0][j])):\n            if out_sigmoid[0][j][k][0] > 0.5:\n                predicted_class[j][k] = 1\n            else:\n                predicted_class[j][k] = 0\n\n\n    prediction_img = np.zeros([img_h, img_w])\n\n    prediction_img[np.where(predicted_class == 0)] = colors_dict[0]\n    prediction_img[np.where(predicted_class == 1)] = colors_dict[1]\n\n    return img_arr[0, ...], prediction_img",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "import os\nfrom datetime import datetime\n\n#function to create csv file\ndef create_csv(results, results_dir='./'):\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(csv_fname, 'w') as f:\n        f.write('ImageId,EncodedPixels,Width,Height\\n')\n\n        for key, value in results.items():\n            f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')\n\n#function to encode predicted ground truth in the required format\ndef rle_encode(img):\n    # Flatten column-wise\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Cycle over test images    \ntest_dir = \"/kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset/test\"\ntest_img_dir = os.path.join(test_dir, 'images', 'img')\n\nimg_filenames = os.listdir(test_img_dir)\n\n#fig, ax = plt.subplots(1, 2, figsize=(8, 8))\n#fig.show()\n\n# Assign a color to each class\ncolors_dict = {}\ncolors_dict[0] = 0          # background\ncolors_dict[1] = 255       # foreground\n\nresults = {}\ncounter = 1\nperc = 0\n\nfor img_filename in img_filenames:\n#if 1:\n  #  img_filename = '492.tif'\n\n    #print % of test done\n    if(counter>perc*len(img_filenames)/100):\n        clear_output()\n        print('-'*perc, '', perc,'%')\n        perc = perc + 1\n    counter = counter + 1\n\n    #prediction of ground truth of the image\n    img, pred = predict_ground_truth(os.path.join(test_img_dir, img_filename))\n    pred = np.uint8(pred/255)\n\n    results[img_filename[:-4]] = rle_encode(pred)\n\n    #ax[0].imshow(np.uint8(img), cmap='gray')\n    #ax[1].imshow(pred, cmap='gray')\n\n    #fig.canvas.draw()\n    #time.sleep(1)\n\ncreate_csv(results)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from IPython.display import FileLink, FileLinks\nFileLinks(\".\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}